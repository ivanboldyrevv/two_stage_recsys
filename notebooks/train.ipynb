{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "Create custom features. Features categories:\n",
    "- User features\n",
    "- Item features\n",
    "- Frequency features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first step: load data and prepared before training/validating/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_uri = \"postgresql+psycopg2://backend:backend@localhost:5432/app_db\"\n",
    "\n",
    "customers = pd.read_sql_table(\"customers\", postgres_uri)\n",
    "articles = pd.read_sql_table(\"articles\", postgres_uri)\n",
    "transactions = pd.read_sql_table(\"transactions\", postgres_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8484 entries, 0 to 8483\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   customer_uuid           8484 non-null   object\n",
      " 1   fn                      8484 non-null   int64 \n",
      " 2   active                  8484 non-null   int64 \n",
      " 3   club_member_status      8484 non-null   object\n",
      " 4   fashion_news_frequency  4359 non-null   object\n",
      " 5   age                     8484 non-null   int64 \n",
      " 6   postal_code             8484 non-null   object\n",
      " 7   customer_id             8484 non-null   int64 \n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 530.4+ KB\n"
     ]
    }
   ],
   "source": [
    "customers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18884 entries, 0 to 18883\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   article_uuid        18884 non-null  object\n",
      " 1   prod_name           18884 non-null  object\n",
      " 2   product_type_no     18884 non-null  int64 \n",
      " 3   product_type_name   18884 non-null  object\n",
      " 4   product_group_no    18884 non-null  int64 \n",
      " 5   product_group_name  18884 non-null  object\n",
      " 6   department_no       18884 non-null  int64 \n",
      " 7   department_name     18884 non-null  object\n",
      " 8   index_code          18884 non-null  object\n",
      " 9   index_name          18884 non-null  object\n",
      " 10  index_group_no      18884 non-null  int64 \n",
      " 11  index_group_name    18884 non-null  object\n",
      " 12  section_no          18884 non-null  int64 \n",
      " 13  section_name        18884 non-null  object\n",
      " 14  garment_group_no    18884 non-null  int64 \n",
      " 15  garment_group_name  18884 non-null  object\n",
      " 16  detail_desc         18884 non-null  object\n",
      " 17  image_id            18884 non-null  int64 \n",
      " 18  article_id          18884 non-null  int64 \n",
      "dtypes: int64(8), object(11)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 319568 entries, 0 to 319567\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   transaction_uuid  319568 non-null  object        \n",
      " 1   t_dat             319568 non-null  datetime64[ns]\n",
      " 2   price             319568 non-null  float64       \n",
      " 3   sales_channel_id  319568 non-null  int64         \n",
      " 4   customer_uuid     319568 non-null  object        \n",
      " 5   article_uuid      319568 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(3)\n",
      "memory usage: 14.6+ MB\n"
     ]
    }
   ],
   "source": [
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepared uset/item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_features = articles[[\n",
    "    \"article_uuid\", \"article_id\", \"product_type_no\", \"product_group_no\", \"department_no\", \"index_code\",\n",
    "    \"index_group_no\", \"section_no\", \"garment_group_no\"\n",
    "]]\n",
    "customer_features = customers[[\n",
    "    \"customer_uuid\", \"customer_id\", \"age\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create interaction matrix (user X item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = transactions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.merge(customer_features, how=\"left\", on=\"customer_uuid\")\n",
    "interactions = interactions.merge(article_features, how=\"left\", on=\"article_uuid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions[[\"customer_id\", \"article_id\", \"t_dat\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 319568 entries, 0 to 319567\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   customer_id  319568 non-null  int64         \n",
      " 1   article_id   319568 non-null  int64         \n",
      " 2   t_dat        319568 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(2)\n",
      "memory usage: 7.3 MB\n"
     ]
    }
   ],
   "source": [
    "interactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Let's divide the data into three parts. The first part is the data for the first stage. The second part is the data for the second stage. The third is the test data for evaluating the overall model.  \n",
    "We will use time intervals. Min date: 2020-03-22; max date: 2020-09-22  \n",
    "- first stage train (2020-03-22/2020-06-22)\n",
    "- first stage validate (2020-06-22/2020-08-22)\n",
    "- second stage train (2020-06-22/2020-08-22)\n",
    "- second stage validate (2020-08-22/2020-09-11)\n",
    "- test (2020-09-11/2020-09-22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary for users from the first training sample to be in all other samples. This code seems to guarantee this condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = \"2020-03-22 00:00:00\"\n",
    "date_1 = \"2020-06-22 00:00:00\"\n",
    "date_2 = \"2020-08-22 00:00:00\" \n",
    "date_3 = \"2020-09-11 00:00:00\" \n",
    "date_4 = \"2020-09-22 00:00:00\"\n",
    "\n",
    "users_in_train = set(interactions[interactions[\"t_dat\"] <= date_1][\"customer_id\"])\n",
    "\n",
    "first_stage_train = interactions[interactions[\"t_dat\"] <= date_1]\n",
    "first_stage_validate = interactions[(interactions[\"t_dat\"] > date_1) & (interactions[\"t_dat\"] <= date_2) & (interactions[\"customer_id\"].isin(users_in_train))]\n",
    "\n",
    "second_stage_train = first_stage_validate\n",
    "second_stage_validate = interactions[(interactions[\"t_dat\"] > date_2) & (interactions[\"t_dat\"] <= date_3) & (interactions[\"customer_id\"].isin(users_in_train))]\n",
    "\n",
    "test = interactions[(interactions[\"t_dat\"] > date_3) & (interactions[\"t_dat\"] <= date_4) & (interactions[\"customer_id\"].isin(users_in_train))]\n",
    "\n",
    "\n",
    "assert len(set(first_stage_validate[\"customer_id\"]) - users_in_train) == 0\n",
    "assert len(set(second_stage_validate[\"customer_id\"]) - users_in_train) == 0\n",
    "assert len(set(test[\"customer_id\"]) - users_in_train) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert str(first_stage_train[\"t_dat\"].min()) == min_date\n",
    "assert str(first_stage_train[\"t_dat\"].max()) == date_1\n",
    "\n",
    "assert str(first_stage_validate[\"t_dat\"].min() - pd.Timedelta(days=1)) == date_1\n",
    "assert str(first_stage_validate[\"t_dat\"].max()) == date_2\n",
    "\n",
    "assert str(second_stage_validate[\"t_dat\"].min() - pd.Timedelta(days=1)) == date_2\n",
    "assert str(second_stage_validate[\"t_dat\"].max()) == date_3\n",
    "\n",
    "assert str(test[\"t_dat\"].min() - pd.Timedelta(days=1)) == date_3\n",
    "assert str(test[\"t_dat\"].max()) == date_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_stage_train length: 163549;\n",
      "first_stage_validate length: 108535;\n",
      "second_stage_train length: 108535;\n",
      "second_stage_validate length: 29766;\n",
      "test length: 14243\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"first_stage_train length: {len(first_stage_train)};\\n\"\n",
    "    f\"first_stage_validate length: {len(first_stage_validate)};\\n\"\n",
    "    f\"second_stage_train length: {len(second_stage_train)};\\n\"\n",
    "    f\"second_stage_validate length: {len(second_stage_validate)};\\n\"\n",
    "    f\"test length: {len(test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 289779;\n",
      "validate length: 15159;\n",
      "test length: 14630;\n"
     ]
    }
   ],
   "source": [
    "min_date = \"2020-03-22 00:00:00\"\n",
    "date_1 = \"2020-09-01 00:00:00\"\n",
    "date_2 = \"2020-09-11 00:00:00\"\n",
    "\n",
    "train = interactions[interactions[\"t_dat\"] <= date_1]\n",
    "validate = interactions[(interactions[\"t_dat\"] > date_1) & (interactions[\"t_dat\"] <= date_2)]\n",
    "test = interactions[(interactions[\"t_dat\"] > date_2)]\n",
    "\n",
    "\n",
    "assert len(set(validate[\"customer_id\"]) - set(train[\"customer_id\"])) == 0\n",
    "assert len(set(test[\"customer_id\"]) - set(train[\"customer_id\"])) == 0\n",
    "\n",
    "print(\n",
    "    f\"train length: {len(train)};\\n\"\n",
    "    f\"validate length: {len(validate)};\\n\"\n",
    "    f\"test length: {len(test)};\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training first stage\n",
    "Create als model which predicts user x items interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\projects\\recsys_v2.0.0\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import implicit\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "from metrics import recall_at_k, precision_at_k\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from typing import List, Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:5000\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://localhost:9000\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"admin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_customers = len(customers)\n",
    "length_articles = len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_coo(interactions, users_len, items_len):\n",
    "    row = interactions[\"customer_id\"].values\n",
    "    col = interactions[\"article_id\"].values\n",
    "    data = np.ones(interactions.shape[0])\n",
    "\n",
    "    return coo_matrix((data, (row, col)), shape=(users_len, items_len), dtype=np.float32)\n",
    "\n",
    "\n",
    "def to_csr(interactions, users_len, items_len):\n",
    "    coo_matrix = to_coo(interactions, users_len, items_len)\n",
    "    csr_matrix = coo_matrix.tocsr()\n",
    "    return csr_matrix\n",
    "\n",
    "\n",
    "train_matrix = to_csr(train, length_customers, length_articles)\n",
    "validate_matrix = to_csr(validate, length_customers, length_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_vector(df, customer_id):\n",
    "    return df[df[\"customer_id\"] == customer_id][\"article_id\"].to_list()\n",
    "\n",
    "\n",
    "def estimate_metrics(model, df, k):\n",
    "    data = df.copy()\n",
    "    data[\"candidates\"] = data[\"customer_id\"].apply(lambda x: model.recommend(x, train_matrix[x], N=k)[0])\n",
    "    recall = []\n",
    "    precision = []\n",
    "    for _, actual, candidates in data.values:\n",
    "        recall.append(recall_at_k(actual, candidates, k=k))\n",
    "        precision.append(precision_at_k(actual, candidates, k=k))\n",
    "    return {\"recall\": np.mean(recall), \"precision\": np.mean(precision)}\n",
    "\n",
    "\n",
    "validate_matrix = pd.DataFrame({\"customer_id\": validate[\"customer_id\"].unique()})\n",
    "validate_matrix[\"actual\"] = validate_matrix[\"customer_id\"].apply(lambda x: items_vector(validate, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model: implicit.als.AlternatingLeastSquares, user_item: csr_matrix):\n",
    "        self.model = model\n",
    "        self.sparse_matrix = user_item\n",
    "\n",
    "    def predict(self, context, model_input: List[int], params: Dict[str, Any] = None) -> List[list]:\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        N = params.get(\"N\", 10)\n",
    "\n",
    "        return self.model.recommend(model_input, self.sparse_matrix[model_input], N=N)\n",
    "\n",
    "    def get_raw_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-04 13:11:12,524] A new study created in memory with name: no-name-fae57277-b9e1-4211-85c1-5a4395e1130b\n",
      "c:\\Users\\user\\projects\\recsys_v2.0.0\\venv\\lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "[I 2025-04-04 13:11:30,103] Trial 0 finished with value: 0.00662960818462962 and parameters: {'factors': 97, 'regularization': 0.00035600175278897346, 'alpha': 10, 'iterations': 300}. Best is trial 0 with value: 0.00662960818462962.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 18.58it/s]\n",
      "2025/04/04 13:11:46 INFO mlflow.models.signature: Inferring model signature from type hints\n",
      "2025/04/04 13:11:46 INFO mlflow.models.signature: Failed to infer output type hint, setting output schema to AnyType. Invalid type hint `list`, it must include a valid element type. Type hints must be a list[...] where collection element type is one of these types: [<class 'int'>, <class 'str'>, <class 'bool'>, <class 'float'>, <class 'bytes'>, <class 'datetime.datetime'>], pydantic BaseModel subclasses, lists and dictionaries of primitive types, or typing.Any. Check https://mlflow.org/docs/latest/model/python_model.html#supported-type-hints for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run ALS w/ recall&precision at: http://localhost:5000/#/experiments/0/runs/ee673e5a5c814f7a802fd18fb829a2f3\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    factors = trial.suggest_int(\"factors\", 10, 200)\n",
    "    regularization = trial.suggest_float(\"regularization\", 1e-6, 1e-2, log=True)\n",
    "    alpha = trial.suggest_int(\"alpha\", -5, 15)\n",
    "    iterations = trial.suggest_int(\"iterations\", 5, 350)\n",
    "\n",
    "    model = implicit.als.AlternatingLeastSquares(\n",
    "        factors=factors,\n",
    "        regularization=regularization,\n",
    "        alpha=alpha,\n",
    "        iterations=iterations,\n",
    "        random_state=42)\n",
    "\n",
    "    model.fit(train_matrix, show_progress=False)\n",
    "\n",
    "    return estimate_metrics(model, validate_matrix, 25)[\"recall\"]\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5, n_warmup_steps=30, interval_steps=10\n",
    "    ),\n",
    ")\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "with mlflow.start_run(run_name=\"ALS w/ recall&precision\"):\n",
    "    als_model = implicit.als.AlternatingLeastSquares()\n",
    "    als_model.fit(train_matrix)\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "\n",
    "    # recall & precision @10\n",
    "    metrics = estimate_metrics(als_model, validate_matrix, 10)\n",
    "    mlflow.log_metric(\"recall_k10\", metrics[\"recall\"])\n",
    "    mlflow.log_metric(\"precision_k10\", metrics[\"precision\"])\n",
    "\n",
    "\n",
    "    xs = np.arange(100, 800, 50)\n",
    "    ys = [estimate_metrics(als_model, validate_matrix, k)[\"recall\"] for k in xs]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=xs,\n",
    "        y=ys,\n",
    "        name=\"recall\"\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=xs,\n",
    "        y=np.gradient(ys) * 10,\n",
    "        name=\"change rate / gradient * 10\",\n",
    "        line=dict(color='red', width=2, dash='dot')\n",
    "    ))\n",
    "\n",
    "    mlflow.log_figure(fig, \"plot_artifacts/change_recall_rate.png\")\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"als_model\",\n",
    "        python_model=ALSModel(als_model, train_matrix)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ALS_Model' already exists. Creating a new version of this model...\n",
      "2025/04/04 13:12:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ALS_Model, version 1\n",
      "Created version '1' of model 'ALS_Model'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1743757946032, current_stage='None', description='', last_updated_timestamp=1743757946032, name='ALS_Model', run_id='ee673e5a5c814f7a802fd18fb829a2f3', run_link='', source='s3://mlflow/0/ee673e5a5c814f7a802fd18fb829a2f3/artifacts/als_model', status='READY', status_message=None, tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.register_model(\n",
    "    \"runs:/ee673e5a5c814f7a802fd18fb829a2f3/als_model\",\n",
    "    \"ALS_Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second stage train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare data for train/validate CatBoost step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match candidates for second step model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_feature(left, right, group_by, agg_col, feature_name):\n",
    "    return left.merge(\n",
    "        right.groupby(by=group_by)[agg_col]\\\n",
    "            .count()\n",
    "            .rename(feature_name) / 1,\n",
    "            how=\"left\",\n",
    "            on=group_by\n",
    "    )\n",
    "\n",
    "\n",
    "def match_candidates(df):\n",
    "    candidates = pd.DataFrame({\"customer_id\": df[\"customer_id\"].unique()})\n",
    "    candidates[\"candidates\"] = candidates[\"customer_id\"].apply(lambda x: als_model.recommend(x, train_matrix[x], 200)[0])\n",
    "\n",
    "    articles = candidates.apply(lambda x: pd.Series(x[\"candidates\"]), axis=1).stack().reset_index(level=1, drop=True)\n",
    "    articles.name = \"article_id\"\n",
    "\n",
    "    return candidates.drop(\"candidates\", axis=1).join(articles)\n",
    "\n",
    "\n",
    "def merge_features(data, customer_features, article_features):\n",
    "    data = data.merge(customer_features, how=\"left\", on=\"customer_id\")\n",
    "    data = data.merge(article_features, how=\"left\", on=\"article_id\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_freq_features(data, right):\n",
    "    data = freq_feature(data, right, [\"article_id\"], \"article_uuid\", \"article_freq\")\n",
    "    data = freq_feature(data, right, [\"customer_id\", \"product_group_no\"], \"article_id\", \"product_group_freq\")\n",
    "    data = freq_feature(data, right, [\"customer_id\", \"index_code\"], \"article_id\", \"index_freq\")\n",
    "    data = freq_feature(data, right, [\"customer_id\", \"garment_group_no\"], \"article_id\", \"garment_group_freq\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = match_candidates(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.copy()\n",
    "X_train = X_train[[\"customer_id\", \"article_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[0:, \"target\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.merge(candidates, how=\"outer\", on=[\"customer_id\", \"article_id\"])\n",
    "X_train = X_train.drop_duplicates(subset=[\"customer_id\", \"article_id\"])\n",
    "X_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 785667 entries, 0 to 855378\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   customer_id  785667 non-null  int64  \n",
      " 1   article_id   785667 non-null  int64  \n",
      " 2   target       785667 non-null  float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 24.0 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = merge_features(X_train, customer_features, article_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 785667 entries, 0 to 785666\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   customer_id       785667 non-null  int64  \n",
      " 1   article_id        785667 non-null  int64  \n",
      " 2   target            785667 non-null  float64\n",
      " 3   customer_uuid     785667 non-null  object \n",
      " 4   age               785667 non-null  int64  \n",
      " 5   article_uuid      785667 non-null  object \n",
      " 6   product_type_no   785667 non-null  int64  \n",
      " 7   product_group_no  785667 non-null  int64  \n",
      " 8   department_no     785667 non-null  int64  \n",
      " 9   index_code        785667 non-null  object \n",
      " 10  index_group_no    785667 non-null  int64  \n",
      " 11  section_no        785667 non-null  int64  \n",
      " 12  garment_group_no  785667 non-null  int64  \n",
      "dtypes: float64(1), int64(9), object(3)\n",
      "memory usage: 77.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 319568 entries, 0 to 319567\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   transaction_uuid  319568 non-null  object        \n",
      " 1   t_dat             319568 non-null  datetime64[ns]\n",
      " 2   price             319568 non-null  float64       \n",
      " 3   sales_channel_id  319568 non-null  int64         \n",
      " 4   customer_uuid     319568 non-null  object        \n",
      " 5   article_uuid      319568 non-null  object        \n",
      " 6   customer_id       319568 non-null  int64         \n",
      " 7   age               319568 non-null  int64         \n",
      " 8   article_id        319568 non-null  int64         \n",
      " 9   product_type_no   319568 non-null  int64         \n",
      " 10  product_group_no  319568 non-null  int64         \n",
      " 11  department_no     319568 non-null  int64         \n",
      " 12  index_code        319568 non-null  object        \n",
      " 13  index_group_no    319568 non-null  int64         \n",
      " 14  section_no        319568 non-null  int64         \n",
      " 15  garment_group_no  319568 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(10), object(4)\n",
      "memory usage: 39.0+ MB\n"
     ]
    }
   ],
   "source": [
    "full_transactions = transactions.copy()\n",
    "full_transactions = full_transactions.merge(customer_features, how=\"left\", on=\"customer_uuid\")\n",
    "full_transactions = full_transactions.merge(article_features, how=\"left\", on=\"article_uuid\")\n",
    "\n",
    "full_transactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = add_freq_features(X_train, full_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 785667 entries, 0 to 785666\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   customer_id         785667 non-null  int64  \n",
      " 1   article_id          785667 non-null  int64  \n",
      " 2   target              785667 non-null  float64\n",
      " 3   customer_uuid       785667 non-null  object \n",
      " 4   age                 785667 non-null  int64  \n",
      " 5   article_uuid        785667 non-null  object \n",
      " 6   product_type_no     785667 non-null  int64  \n",
      " 7   product_group_no    785667 non-null  int64  \n",
      " 8   department_no       785667 non-null  int64  \n",
      " 9   index_code          785667 non-null  object \n",
      " 10  index_group_no      785667 non-null  int64  \n",
      " 11  section_no          785667 non-null  int64  \n",
      " 12  garment_group_no    785667 non-null  int64  \n",
      " 13  article_freq        785667 non-null  float64\n",
      " 14  product_group_freq  768562 non-null  float64\n",
      " 15  index_freq          776993 non-null  float64\n",
      " 16  garment_group_freq  746932 non-null  float64\n",
      "dtypes: float64(5), int64(9), object(3)\n",
      "memory usage: 101.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>target</th>\n",
       "      <th>customer_uuid</th>\n",
       "      <th>age</th>\n",
       "      <th>article_uuid</th>\n",
       "      <th>product_type_no</th>\n",
       "      <th>product_group_no</th>\n",
       "      <th>department_no</th>\n",
       "      <th>index_code</th>\n",
       "      <th>index_group_no</th>\n",
       "      <th>section_no</th>\n",
       "      <th>garment_group_no</th>\n",
       "      <th>article_freq</th>\n",
       "      <th>product_group_freq</th>\n",
       "      <th>index_freq</th>\n",
       "      <th>garment_group_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>489044</th>\n",
       "      <td>7412</td>\n",
       "      <td>16607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b7f4342d-db64-48e8-a673-3f539d8bc095</td>\n",
       "      <td>41</td>\n",
       "      <td>b025d58a-3f76-4ad6-a92d-95b34795f41f</td>\n",
       "      <td>265</td>\n",
       "      <td>6</td>\n",
       "      <td>1641</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1005</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        customer_id  article_id  target                         customer_uuid  \\\n",
       "489044         7412       16607     0.0  b7f4342d-db64-48e8-a673-3f539d8bc095   \n",
       "\n",
       "        age                          article_uuid  product_type_no  \\\n",
       "489044   41  b025d58a-3f76-4ad6-a92d-95b34795f41f              265   \n",
       "\n",
       "        product_group_no  department_no index_code  index_group_no  \\\n",
       "489044                 6           1641          A               1   \n",
       "\n",
       "        section_no  garment_group_no  article_freq  product_group_freq  \\\n",
       "489044          18              1005         125.0                 2.0   \n",
       "\n",
       "        index_freq  garment_group_freq  \n",
       "489044        21.0                36.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(data, freq_columns, drop_columns):\n",
    "    for col_name in freq_columns:\n",
    "        data[col_name].fillna(0, inplace=True)\n",
    "    data = data.drop(columns=drop_columns)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4204\\328287430.py:3: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_columns = [\"article_freq\", \"product_group_freq\", \"index_freq\", \"garment_group_freq\"]\n",
    "drop_columns = [\"customer_uuid\", \"article_uuid\"]\n",
    "\n",
    "X_train = prepare(X_train, freq_columns, drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[[\"target\"]]\n",
    "X_train = X_train.drop(columns=[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(customer_id, df, k=5):\n",
    "    return df[df[\"customer_id\"] == customer_id].\\\n",
    "                sort_values(\"proba\", ascending=False).head(k)[\"article_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-04 13:12:52,874] A new study created in memory with name: no-name-3d93526c-10ba-4cd1-acc0-4f3895ac9776\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4204\\1677756698.py:24: FutureWarning:\n",
      "\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4204\\328287430.py:3: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "[I 2025-04-04 13:14:32,570] Trial 0 finished with value: 0.007861109497883146 and parameters: {'iterations': 354, 'max_depth': 12, 'learning_rate': 0.08806956439602256, 'l2_leaf_reg': 1.0761922079569235}. Best is trial 0 with value: 0.007861109497883146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run wise-dove-968 at: http://localhost:5000/#/experiments/0/runs/bccdf6ab2036401c9672e6a1ee29aea3\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4204\\328287430.py:3: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "2025/04/04 13:16:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run catboost/match candidates + rerank at: http://localhost:5000/#/experiments/0/runs/f1dc3ca28b234cc393696d10a2db1d8d\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "def match_rerank(data, rerank_model):\n",
    "    data = match_candidates(validate)\n",
    "\n",
    "    data = merge_features(data, customer_features, article_features)\n",
    "    data = add_freq_features(data, full_transactions)\n",
    "    data = prepare(data, freq_columns, drop_columns)\n",
    "\n",
    "    data[\"proba\"] = rerank_model.predict(data, prediction_type=\"Probability\")[:, 1]\n",
    "\n",
    "    compared = pd.DataFrame({\"customer_id\": validate[\"customer_id\"].unique()})\n",
    "    compared[\"reranked\"] = compared[\"customer_id\"].apply(lambda x: rerank(x, data, 25))\n",
    "    compared[\"actual\"] = compared[\"customer_id\"].apply(lambda x: validate[validate[\"customer_id\"] == x][\"article_id\"].to_list())\n",
    "\n",
    "    return compared\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    with mlflow.start_run():\n",
    "        params = {\n",
    "            \"objective\": \"Logloss\",\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 6, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-5, 1e2),\n",
    "            \"task_type\": \"GPU\"\n",
    "        }\n",
    "\n",
    "        catboost_model = CatBoost(\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        catboost_model.fit(X_train, y_train, cat_features=[\"index_code\"], verbose=False)\n",
    "        candidates = match_rerank(validate, catboost_model)\n",
    "\n",
    "        recall = []\n",
    "        precision = []\n",
    "\n",
    "        for _, reranked, actual in candidates.values:\n",
    "            recall.append(recall_at_k(actual, reranked, 25))\n",
    "            precision.append(precision_at_k(actual, reranked, 25))\n",
    "\n",
    "        return np.mean(recall)\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5, n_warmup_steps=30, interval_steps=10\n",
    "    )\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "with mlflow.start_run(run_name=\"catboost/match candidates + rerank\"):\n",
    "    catboost_model = CatBoost(params=study.best_params)\n",
    "    catboost_model.fit(X_train, y_train, cat_features=[\"index_code\"], verbose=False)\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "\n",
    "    candidates = match_rerank(validate, catboost_model)\n",
    "\n",
    "    recall = []\n",
    "    precision = []\n",
    "\n",
    "    for _, reranked, actual in candidates.values:\n",
    "        recall.append(recall_at_k(actual, reranked, 25))\n",
    "        precision.append(precision_at_k(actual, reranked, 25))\n",
    "\n",
    "    mean_recall = np.mean(recall)\n",
    "    mean_precision = np.mean(precision)\n",
    "\n",
    "    mlflow.log_metric(\"recall\", mean_recall)\n",
    "    mlflow.log_metric(\"precision\", mean_precision)\n",
    "    \n",
    "    mlflow.catboost.log_model(\n",
    "        catboost_model,\n",
    "        artifact_path=\"catboost_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'CatBoost_Model'.\n",
      "2025/04/04 14:09:08 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: CatBoost_Model, version 1\n",
      "Created version '1' of model 'CatBoost_Model'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1743761348880, current_stage='None', description='', last_updated_timestamp=1743761348880, name='CatBoost_Model', run_id='f1dc3ca28b234cc393696d10a2db1d8d', run_link='', source='s3://mlflow/0/f1dc3ca28b234cc393696d10a2db1d8d/artifacts/catboost_model', status='READY', status_message=None, tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.register_model(\n",
    "    \"runs:/f1dc3ca28b234cc393696d10a2db1d8d/catboost_model\",\n",
    "    \"CatBoost_Model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
